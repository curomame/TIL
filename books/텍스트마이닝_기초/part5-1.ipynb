{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "7532\n",
      "\n",
      "{'comp.sys.mac.hardware', 'sci.space', 'alt.atheism', 'comp.graphics', 'talk.politics.mideast', 'sci.med', 'rec.motorcycles', 'sci.crypt', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'comp.sys.ibm.pc.hardware', 'comp.windows.x', 'soc.religion.christian', 'rec.autos', 'rec.sport.baseball', 'talk.politics.misc', 'talk.religion.misc', 'talk.politics.guns', 'misc.forsale', 'sci.electronics'}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(len(newsgroups_train.data))\n",
    "print(len(newsgroups_test.data))\n",
    "print()\n",
    "print(set(newsgroups_train.target_names))\n",
    "print(set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2000)\n",
      "\n",
      "(7532, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000, min_df = 5, max_df=0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train) # train set을 변환\n",
    "print(X_train_cv.shape)\n",
    "print()\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print(X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 0\n",
      "000 0\n",
      "01 0\n",
      "02 0\n",
      "03 0\n",
      "04 0\n",
      "05 0\n",
      "06 0\n",
      "0d 0\n",
      "0t 0\n",
      "10 0\n",
      "100 0\n",
      "11 0\n",
      "12 0\n",
      "128 0\n",
      "13 0\n",
      "14 0\n",
      "145 0\n",
      "15 0\n",
      "150 0\n",
      "16 0\n",
      "17 0\n",
      "18 0\n",
      "19 0\n",
      "1988 0\n",
      "1989 0\n",
      "1990 0\n",
      "1991 0\n",
      "1992 0\n",
      "1993 0\n",
      "1d9 0\n",
      "1st 0\n",
      "1t 0\n",
      "20 0\n",
      "200 0\n",
      "21 0\n",
      "22 0\n",
      "23 0\n",
      "24 0\n",
      "25 0\n",
      "250 0\n",
      "256 0\n",
      "26 0\n",
      "27 0\n",
      "28 0\n",
      "29 0\n",
      "2di 0\n",
      "2nd 0\n",
      "2tm 0\n",
      "30 0\n",
      "300 0\n",
      "31 0\n",
      "32 0\n",
      "33 0\n",
      "34 0\n",
      "34u 0\n",
      "35 0\n",
      "36 0\n",
      "37 0\n",
      "38 0\n",
      "386 0\n",
      "39 0\n",
      "3d 0\n",
      "3l 0\n",
      "3rd 0\n",
      "3t 0\n",
      "40 0\n",
      "400 0\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 0\n",
      "46 0\n",
      "47 0\n",
      "48 0\n",
      "486 0\n",
      "49 0\n",
      "4t 0\n",
      "50 0\n",
      "500 0\n",
      "51 0\n",
      "52 0\n",
      "53 0\n",
      "54 0\n",
      "55 0\n",
      "56 0\n",
      "57 0\n",
      "58 0\n",
      "59 0\n",
      "5u 0\n",
      "60 0\n",
      "600 0\n",
      "61 0\n",
      "63 0\n",
      "64 0\n",
      "65 0\n",
      "66 0\n",
      "68 0\n",
      "6ei 0\n"
     ]
    }
   ],
   "source": [
    "for word, count in zip(\n",
    "  cv.get_feature_names_out()[:100], X_train_cv.toarray()[0, :100]\n",
    "):\n",
    "  print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6248011313417006\n",
      "0.5031864046733935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#분류기 선언\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "#train set을 이용해 분류기 (classifier) 학습\n",
    "NB_clf.fit(X_train_cv, Y_train)\n",
    "\n",
    "#train set 에 대한 예측 정확도 확인\n",
    "\n",
    "print(NB_clf.score(X_train_cv, Y_train))\n",
    "print(NB_clf.score(X_test_cv, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy. 7\n",
      "I'm not familiar at all with the format of these \"X-Face:\" thingies, but\n",
      "after seeing them in some folks' headers, I've *got* to *see* them (and\n",
      "maybe make one of my own)!\n",
      "\n",
      "I've got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\n",
      "and I've managed to compile [un]compface too... but now that I'm *looking*\n",
      "for them, I can't seem to find any X-Face:'s in anyones news headers!  :-(\n",
      "\n",
      "Could you, would you, please send me your \"X-Face:\" header?\n",
      "\n",
      "I *know* I'll probably get a little swamped, but I can handle it.\n",
      "\n",
      "\t...I hope. 5\n",
      "\n",
      "[7 8]\n",
      "rec.autos rec.motorcycles\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0],Y_test[0])\n",
    "print(X_test[1],Y_test[1])\n",
    "\n",
    "print()\n",
    "\n",
    "pred = NB_clf.predict(X_test_cv[:2])\n",
    "\n",
    "print(pred)\n",
    "print(\n",
    "  newsgroups_train.target_names[pred[0]],\n",
    "  newsgroups_train.target_names[pred[1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7247657769135584\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#CounterVectorizer와 동일한 인수 사용\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train) # train set을 반환\n",
    "X_test_tfidf = tfidf.transform(X_test) #test set을 반환\n",
    "\n",
    "#tfidf train set을 이용해 분류기를 새로 학습\n",
    "NB_clf.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "#train set에 대한 예측 정확도 확인\n",
    "print(NB_clf.score(X_train_tfidf, Y_train))\n",
    "\n",
    "#test set에 대한 예측 정확도 확인\n",
    "print(NB_clf.score(X_test_tfidf, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39m#카테고리와 영향이 큰 특성 10개를 출력\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (category, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(feature_names[top10])))\n\u001b[0;32m---> 12\u001b[0m top10_features(NB_clf, tfidf, newsgroups_train\u001b[39m.\u001b[39;49mtarget_names)\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mtop10_features\u001b[0;34m(classifier, vectorizer, categories)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtop10_features\u001b[39m(classifier, vectorizer, categories):\n\u001b[0;32m----> 4\u001b[0m   feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(vectorizer\u001b[39m.\u001b[39;49mget_feature_names())\n\u001b[1;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m i, category \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(categories):\n\u001b[1;32m      6\u001b[0m     \u001b[39m#역순으로 정렬하기 위해 계수에 음수를 취해 정렬 후 앞에서부터 10개 값을 반환\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     top10 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(\u001b[39m-\u001b[39mclassifier\u001b[39m.\u001b[39mcoef_[i])[:\u001b[39m10\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "  feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "  for i, category in enumerate(categories):\n",
    "    #역순으로 정렬하기 위해 계수에 음수를 취해 정렬 후 앞에서부터 10개 값을 반환\n",
    "    top10 = np.argsort(-classifier.coef_[i])[:10]\n",
    "    \n",
    "    #카테고리와 영향이 큰 특성 10개를 출력\n",
    "    print(\"%s: %s\" % (category, \", \".join(feature_names[top10])))\n",
    "    \n",
    "top10_features(NB_clf, tfidf, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9331801308113842\n",
      "0.5128783855549655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leecoder/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# count vector 에 대해 regression을 해서 NB와 비교\n",
    "LR_clf = LogisticRegression()\n",
    "\n",
    "#train data를 이용해 분류기 학습\n",
    "LR_clf.fit(X_train_cv, Y_train)\n",
    "\n",
    "#train data에 대한 예측 정확도\n",
    "print(LR_clf.score(X_train_cv, Y_train))\n",
    "\n",
    "#test data에 대한 예측 정확도\n",
    "print(LR_clf.score(X_test_cv, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8129750751281598\n",
      "0.5715613382899628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier() # 릿ㅂ지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, Y_train) # train set을 이용해 분류기 학습\n",
    "\n",
    "print(ridge_clf.score(X_train_tfidf, Y_train)) # train set에 대한 예측 정확도\n",
    "print(ridge_clf.score(X_test_tfidf, Y_test)) # test set에 대한 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 0.6394167034909413\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ridge, X_val_ridge, y_train_ridge, y_val_ridge = train_test_split(\n",
    "  X_train_tfidf, Y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "max_score = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 10, 0.1) : #alpha를 0.1부터 10까지 0.1씩 증가\n",
    "  ridge_clf = RidgeClassifier(alpha=alpha) #릿지 분류기 선언\n",
    "  ridge_clf.fit(X_train_ridge, y_train_ridge) #학습\n",
    "  \n",
    "  #검정 데이터셋에 대해 정확도를 측정\n",
    "  score = ridge_clf.score(X_val_ridge, y_val_ridge)\n",
    "  if score > max_score:\n",
    "    max_score = score\n",
    "    max_alpha = alpha\n",
    "\n",
    "\n",
    "print(max_alpha, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7983913735195334\n",
      "0.5758098778544876\n"
     ]
    }
   ],
   "source": [
    "ridge_clf = RidgeClassifier(alpha = 2.1)\n",
    "ridge_clf.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "print(ridge_clf.score(X_train_tfidf, Y_train))\n",
    "print(ridge_clf.score(X_test_tfidf, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top10_features(ridge_clf, tfidf, newsgroups_train\u001b[39m.\u001b[39;49mtarget_names)\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mtop10_features\u001b[0;34m(classifier, vectorizer, categories)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtop10_features\u001b[39m(classifier, vectorizer, categories):\n\u001b[0;32m----> 4\u001b[0m   feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(vectorizer\u001b[39m.\u001b[39;49mget_feature_names())\n\u001b[1;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m i, category \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(categories):\n\u001b[1;32m      6\u001b[0m     \u001b[39m#역순으로 정렬하기 위해 계수에 음수를 취해 정렬 후 앞에서부터 10개 값을 반환\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     top10 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(\u001b[39m-\u001b[39mclassifier\u001b[39m.\u001b[39mcoef_[i])[:\u001b[39m10\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "top10_features(ridge_clf, tfidf, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6841965706204702\n",
      "0.5497875730217737\n",
      "2999\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Lasso는 동일한 Logistic Regression을 사용하면서 매개변수로 지정\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear', C=1)\n",
    "\n",
    "lasso_clf.fit(X_train_tfidf, Y_train) # train data로 학습\n",
    "\n",
    "print(lasso_clf.score(X_train_tfidf, Y_train)) # train data에 대한 예측 정확도\n",
    "print(lasso_clf.score(X_test_tfidf, Y_test)) # test data에 대한 예측 정확도\n",
    "\n",
    "#계수(coefficient)가 0이 아닌 것만 출력\n",
    "print(np.sum(lasso_clf.coef_ != 0 ))\n",
    "print(X_train_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9724235460491426\n",
      "0.3856877323420074\n",
      "0.9724235460491426\n",
      "0.5339883165161976\n",
      "0.8526604207176949\n",
      "0.5155337227827934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "print(tree.score(X_train_tfidf, Y_train))\n",
    "print(tree.score(X_test_tfidf, Y_test))\n",
    "\n",
    "forest = RandomForestClassifier(random_state=7)\n",
    "forest.fit(X_train_tfidf, Y_train)\n",
    "print(forest.score(X_train_tfidf, Y_train))\n",
    "print(forest.score(X_test_tfidf, Y_test))\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=7)\n",
    "gb.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "print(gb.score(X_train_tfidf, Y_train))\n",
    "print(gb.score(X_test_tfidf, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "god: 0.025809766377474817, windows: 0.025013955848858276, sale: 0.02454447306012166, bike: 0.023908116678280248, car: 0.021296532916442883, space: 0.019275009188592773, encryption: 0.019223838663201, hockey: 0.016838620487237527, gun: 0.01543072867889741, israel: 0.0151094571916706, mac: 0.012025547333926337, apple: 0.010931917122283245, clipper: 0.009809860691815418, window: 0.00958436709485705, graphics: 0.00939651873996847, team: 0.009216766601239941, baseball: 0.007619106003736491, banks: 0.007584668919983667, turkish: 0.007077320005812768, motif: 0.0069283677580132715, israeli: 0.006883188058373934, server: 0.006726055270159761, shipping: 0.006599223666254451, circuit: 0.006469524109890036, game: 0.0063933629054823745, jesus: 0.0058978036322069665, key: 0.005714170103936138, cars: 0.005656518428482097, widget: 0.005275802409356186, dod: 0.005265490762716681, nsa: 0.005085899669435628, orbit: 0.004981398761117598, guns: 0.004782315661657503, christians: 0.004717348043021744, atheism: 0.004671429060468695, church: 0.004671210677589661, controller: 0.004467313548523766, christian: 0.004404922221410116, armenian: 0.004397920143974365, nhl: 0.00427011570821842, "
     ]
    }
   ],
   "source": [
    "sorted_feature_importances = sorted(\n",
    "  zip(tfidf.get_feature_names_out(), gb.feature_importances_),\n",
    "  key = lambda x:x[1],\n",
    "  reverse=True\n",
    ")\n",
    "\n",
    "for feature, value in sorted_feature_importances[:40]:\n",
    "  print(\"%s: %s\" % (feature, value), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
