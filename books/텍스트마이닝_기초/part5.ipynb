{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "7532\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#학습 데이터셋 가져옴\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers','footers','quotes'))\n",
    "\n",
    "#평가 데이터 셋 가져옴\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers','footers','quotes'))\n",
    "\n",
    "print(len(newsgroups_train.data))\n",
    "print(len(newsgroups_test.data))\n",
    "\n",
    "print(newsgroups_train.target_names)\n",
    "print(set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "7\n",
      "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])\n",
    "print(newsgroups_train.target[0])\n",
    "print(newsgroups_test.data[0])\n",
    "print(newsgroups_test.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2000)\n",
      "(7532, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000,min_df=5,max_df=0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train) #train set을 변환\n",
    "print(X_train_cv.shape)\n",
    "\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print(X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0, 000 : 0, 0000 : 0, 001 : 0, 002 : 0, 01 : 0, 011 : 0, 014 : 0, 02 : 0, 0200 : 0, 02106 : 0, 02139 : 0, 03 : 0, 030 : 0, 0300 : 0, 033 : 0, 04 : 0, 040 : 0, 05 : 0, 0500 : 0, 06 : 0, 0666 : 0, 07 : 0, 08 : 0, 081 : 0, 0863 : 0, 089 : 0, 09 : 0, 0_ : 0, 0a : 0, 0b : 0, 0c : 0, 0d : 0, 0f : 0, 0g : 0, 0h : 0, 0i : 0, 0iv : 0, 0j : 0, 0k : 0, 0l : 0, 0m : 0, 0m75u : 0, 0n : 0, 0o : 0, 0p : 0, 0q : 0, 0qax : 0, 0qq : 0, 0r : 0, 0s : 0, 0t : 0, 0tbxn : 0, 0tbxom : 0, 0tq : 0, 0u : 0, 0v : 0, 0w : 0, 0x : 0, 0y : 0, 0z : 0, 10 : 0, 100 : 0, 1000 : 0, 100k : 0, 101 : 0, 1010 : 0, 102 : 0, 1024 : 0, 1024x768 : 0, 1024x768x256 : 0, 1026 : 0, 1029 : 0, 103 : 0, 1030 : 0, 1036 : 0, 104 : 0, 1047 : 0, 105 : 0, 1054 : 0, 106 : 0, 1069 : 0, 107 : 0, 1070 : 0, 1074 : 0, 108 : 0, 109 : 0, 1091 : 0, 10k : 0, 10mb : 0, 10mhz : 0, 10th : 0, 10w : 0, 11 : 0, 110 : 0, 1100 : 0, 111 : 0, 1111 : 0, 1113 : 0, 112 : 0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(\n",
    "  cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0,:100]\n",
    "):\n",
    "  print(word,':',count, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.790\n",
      "Test set score: 0.634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 분류기 선언\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "#train set을 이용해 분류기 classifier 를 학습\n",
    "NB_clf.fit(X_train_cv,Y_train)\n",
    "\n",
    "#train set에 대한 예측 정도를 확인\n",
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv,Y_train)))\n",
    "\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy. 7 //\n",
      "I'm not familiar at all with the format of these \"X-Face:\" thingies, but\n",
      "after seeing them in some folks' headers, I've *got* to *see* them (and\n",
      "maybe make one of my own)!\n",
      "\n",
      "I've got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\n",
      "and I've managed to compile [un]compface too... but now that I'm *looking*\n",
      "for them, I can't seem to find any X-Face:'s in anyones news headers!  :-(\n",
      "\n",
      "Could you, would you, please send me your \"X-Face:\" header?\n",
      "\n",
      "I *know* I'll probably get a little swamped, but I can handle it.\n",
      "\n",
      "\t...I hope. 5 //\n",
      "[7 1] //PRED//\n",
      "rec.autos comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0],Y_test[0],'//')\n",
    "print(X_test[1],Y_test[1],'//')\n",
    "\n",
    "pred = NB_clf.predict(X_test_cv[:2])\n",
    "print(pred,'//PRED//')\n",
    "\n",
    "print(\n",
    "  newsgroups_train.target_names[pred[0]],\n",
    "  newsgroups_train.target_names[pred[1]]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.725\n",
      "Test set score: 0.078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CountVectorizer와 동일한 인수를 사용\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.fit_transform(X_test)\n",
    "\n",
    "#tfidf train set을 이용해 분류기를 새로 학습\n",
    "NB_clf.fit(X_train_tfidf,Y_train)\n",
    "\n",
    "#train set에 대한 예측정확도를 확인\n",
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf,Y_train)))\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39m#카테고리와 영향이 큰 특성 10개를 출력\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (category, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(feature_names[top10])) )\n\u001b[0;32m---> 12\u001b[0m top10_features(NB_clf,tfidf,newsgroups_train\u001b[39m.\u001b[39;49mtarget_names)\n",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m, in \u001b[0;36mtop10_features\u001b[0;34m(classifier, vectorizer, categories)\u001b[0m\n\u001b[1;32m      4\u001b[0m feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(vectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i, category \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(categories):\n\u001b[1;32m      7\u001b[0m   \u001b[39m#역순으로 정렬하기 위해 계수에 음수를 취해 정렬 후 앞에서부터 10개의 값을 반환\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m   top10 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(\u001b[39m-\u001b[39mclassifier\u001b[39m.\u001b[39;49mcoef_[i])[:\u001b[39m10\u001b[39m]\n\u001b[1;32m      9\u001b[0m   \u001b[39m#카테고리와 영향이 큰 특성 10개를 출력\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (category, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(feature_names[top10])) )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top10_features(classifier,vectorizer, categories):\n",
    "  feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "  \n",
    "  for i, category in enumerate(categories):\n",
    "    #역순으로 정렬하기 위해 계수에 음수를 취해 정렬 후 앞에서부터 10개의 값을 반환\n",
    "    top10 = np.argsort(-classifier.coef_[i])[:10]\n",
    "    #카테고리와 영향이 큰 특성 10개를 출력\n",
    "    print(\"%s %s\" % (category, \", \".join(feature_names[top10])) )\n",
    "    \n",
    "top10_features(NB_clf,tfidf,newsgroups_train.target_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792\n",
      "0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leecoder/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#sklearn이 제공하는 logistic regression을 사용\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "\n",
    "#train data를 이용해 분류기를 학습\n",
    "LR_clf.fit(X_train_cv,Y_train)\n",
    "\n",
    "#train data에 대한 예측 정확도\n",
    "print('{:.3f}'.format(LR_clf.score(X_train_tfidf,Y_train)))\n",
    "print('{:.3f}'.format(LR_clf.score(X_test_tfidf,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m RidgeClassifier\n\u001b[1;32m      3\u001b[0m ridge_clf \u001b[39m=\u001b[39m RidgeClassifier() \u001b[39m#릿지 분류기 선언\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ridge_clf\u001b[39m.\u001b[39;49mfit(X_train_tfidf,X_train_tfidf) \u001b[39m#학습\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ridge_clf\u001b[39m.\u001b[39mscore(X_train_tfidf,Y_train)))\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ridge_clf\u001b[39m.\u001b[39mscore(X_test_tfidf,Y_test)))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1422\u001b[0m, in \u001b[0;36mRidgeClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit Ridge classifier model.\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m \n\u001b[1;32m   1400\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m    Instance of the estimator.\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1422\u001b[0m X, y, sample_weight, Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_data(X, y, sample_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver)\n\u001b[1;32m   1424\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, Y, sample_weight\u001b[39m=\u001b[39msample_weight)\n\u001b[1;32m   1425\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1180\u001b[0m, in \u001b[0;36m_RidgeClassifierMixin._prepare_data\u001b[0;34m(self, X, y, sample_weight, solver)\u001b[0m\n\u001b[1;32m   1171\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m   1172\u001b[0m     X,\n\u001b[1;32m   1173\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     y_numeric\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1177\u001b[0m )\n\u001b[1;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_binarizer \u001b[39m=\u001b[39m LabelBinarizer(pos_label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, neg_label\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1180\u001b[0m Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_label_binarizer\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[1;32m   1181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_binarizer\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:334\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \n\u001b[1;32m    317\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39mtransform(y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:304\u001b[0m, in \u001b[0;36mLabelBinarizer.fit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_ \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultioutput\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_:\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMultioutput target data is not supported with label binarization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[1;32m    307\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    308\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my has 0 samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y)\n",
      "\u001b[0;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf,X_train_tfidf) #학습\n",
    "\n",
    "print('{:.3f}'.format(ridge_clf.score(X_train_tfidf,Y_train)))\n",
    "print('{:.3f}'.format(ridge_clf.score(X_test_tfidf,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.500 0.640\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ridge, X_val_ridge, Y_train_ridge, Y_val_ridge = train_test_split(\n",
    "  X_train_tfidf, Y_train, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "max_score = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 10, 0.1): #alpha를 0.1부터 10까지 0.1씩 증가\n",
    "  ridge_clf = RidgeClassifier(alpha = alpha) #릿지 분류기 선언\n",
    "  ridge_clf.fit(X_train_ridge, Y_train_ridge) #학습\n",
    "  \n",
    "  # 검정 데이터 셋에 대해 정확도를 측정\n",
    "  score = ridge_clf.score(X_val_ridge, Y_val_ridge)\n",
    "  \n",
    "  if score > max_score: #정확도가 이전의 정확도 최댓값보다 크면 최댓값을 변경\n",
    "    max_score = score\n",
    "    max_alpha = alpha\n",
    "    \n",
    "print('{:.3f} {:.3f}'.format(max_alpha, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785\n",
      "0.074\n"
     ]
    }
   ],
   "source": [
    "ridge_clf = RidgeClassifier(alpha = 3.5) #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, Y_train) #학습\n",
    "\n",
    "print('{:.3f}'.format(ridge_clf.score(X_train_tfidf, Y_train)))\n",
    "print('{:.3f}'.format(ridge_clf.score(X_test_tfidf, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
