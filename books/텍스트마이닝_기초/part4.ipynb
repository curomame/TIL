{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count: 2000\n",
      "sample ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "neg/cv000_29416.txt\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading movie_revies: Package 'movie_revies' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_revies')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count:',len(movie_reviews.fileids()))\n",
    "print('sample',movie_reviews.fileids()[:10])\n",
    "\n",
    "fileid = movie_reviews.fileids()[0]\n",
    "print(fileid)\n",
    "\n",
    "#첫번째 문서의 내용을 200자까지만 출력\n",
    "print(movie_reviews.raw(fileid)[:200])\n",
    "\n",
    "#첫번째 문서를 sentence tokenize한 결과 중 앞 두 문자\n",
    "print(movie_reviews.sents(fileid)[:2])\n",
    "\n",
    "#첫번째 문서를 word tokenize한 결과 중 앞 20개 단어\n",
    "print(movie_reviews.words(fileid)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n"
     ]
    }
   ],
   "source": [
    "documents = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n",
    "print(documents[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of ',:77717, count of 'the:76529, count of '.:65876, count of 'a:38106, count of 'and:35576, count of 'of:34123, count of 'to:31937, count of '':30585, count of 'is:25195, count of 'in:21822, "
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "\n",
    "for text in documents:\n",
    "  for word in text:\n",
    "    word_count[word] = word_count.get(word,0) + 1\n",
    "    \n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "\n",
    "for word in sorted_features[:10]:\n",
    "  print(f\"count of '{word}:{word_count[word]}\",end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features: 39208\n",
      "count of 'film: 9517, count of 'one: 5852, count of 'movie: 5771, count of 'like: 3690, count of 'even: 2565, count of 'good: 2411, count of 'time: 2411, count of 'story: 2169, count of 'would: 2109, count of 'much: 2049, "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords #일반적 분석 대상이 아닌 words\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w]{3,}\") #3글자 이상의 단어만 추출, 정규 표현식으로 토크나이저 정의\n",
    "eng_stops = set(stopwords.words('english'))\n",
    "\n",
    "#words 대신 raw로 원문 가져옴\n",
    "\n",
    "documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "#stopwords의 적용과 토큰화를 동시에 수행.\n",
    "\n",
    "tokens = [[token for token in tokenizer.tokenize(doc) if token not in eng_stops] for doc in documents]\n",
    "\n",
    "word_count = {}\n",
    "for text in tokens:\n",
    "  for word in text:\n",
    "    word_count[word] = word_count.get(word,0) + 1\n",
    "    \n",
    "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
    "\n",
    "print('num of features:',len(sorted_features))\n",
    "for word in sorted_features[:10]:\n",
    "  print(f\"count of '{word}: {word_count[word]}\",end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도가 높은 상위 1000개ㅔ의 단어만 추출해 features를 구성\n",
    "word_features = sorted_features[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 주어진 document를 feature로 변환하는 함수, word_features를 사용\n",
    "\n",
    "def document_features(document, word_features):\n",
    "  word_count={}\n",
    "  for word in document:\n",
    "    word_count[word] = word_count.get(word,0) + 1\n",
    "    \n",
    "  features = []\n",
    "  # word features의 단어에 대해 계산된 빈도수를 feature에 추가\n",
    "  for word in word_features:\n",
    "    features.append(word_count.get(word,0))\n",
    "  return features\n",
    "\n",
    "word_features_ex = ['one','two','teeb','couples','solo']\n",
    "doc_ex = ['two','two','couples']\n",
    "print(document_features(doc_ex, word_features_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(film),6, (one),3, (movie),6, (like),3, (even),3, (good),2, (time),0, (story),0, (would),1, (much),0, (character),2, (also),1, (get),3, (two),2, (well),1, (characters),1, (first),0, (see),2, (way),3, (make),5, "
     ]
    }
   ],
   "source": [
    "feature_sets = [document_features(d, word_features) for d in tokens]\n",
    "\n",
    "#첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력\n",
    "\n",
    "for i in range(20):\n",
    "  print(f'({word_features[i]}),{feature_sets[0][i]}',end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷 런으로 카운트 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비, movie_reviews.raw()를 사용해 raw text 추출\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(vocabulary=['film', 'one', 'movie', 'like', 'even', 'good',\n",
      "                            'time', 'story', 'would', 'much', 'character',\n",
      "                            'also', 'get', 'two', 'well', 'characters', 'first',\n",
      "                            'see', 'way', 'make', 'life', 'really', 'films',\n",
      "                            'plot', 'little', 'people', 'could', 'scene', 'man',\n",
      "                            'bad', ...])\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#cv = CountVectorizer() 모든 매개변수에 기본값을 사용하는 경우\n",
    "\n",
    "# 앞에서 생성한 word_features 로 특성 집합을 지정하는 경우\n",
    "\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "\n",
    "# 특성 집합을 정하지 않고 최대 특성의 수를 지정하는 경우\n",
    "\n",
    "# cb = CountVectorizer(max_features=1000)\n",
    "\n",
    "print(cv)\n",
    "print(type(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film' 'one' 'movie' 'like' 'even' 'good' 'time' 'story' 'would' 'much'\n",
      " 'character' 'also' 'get' 'two' 'well' 'characters' 'first' 'see' 'way'\n",
      " 'make']\n",
      "['film', 'one', 'movie', 'like', 'even', 'good', 'time', 'story', 'would', 'much', 'character', 'also', 'get', 'two', 'well', 'characters', 'first', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "reviews_cv = cv.fit_transform(reviews) # reviews를 이용해 count vector를 학습하고, 변환\n",
    "print(cv.get_feature_names_out()[:20])\n",
    "print(word_features[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#type of count vectors: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "#type of count vectors: (2000, 1000)\n",
      "\n",
      "  (0, 0)\t6\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t6\n",
      "  (0, 3)\t3\n",
      "  (0, 4)\t3\n",
      "  (0, 5)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "print('#type of count vectors:',type(reviews_cv))\n",
    "print('#type of count vectors:',reviews_cv.shape)\n",
    "print()\n",
    "print(reviews_cv[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 254135 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3, 6, 3, 3, 2, 0, 0, 1, 0, 2, 1, 3, 2, 1, 1, 0, 2, 3, 5]\n",
      "[6 3 6 3 3 2 0 0 1 0 2 1 3 2 1 1 0 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(feature_sets[0][:20])\n",
    "\n",
    "print(reviews_cv.toarray()[0,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film:6, one:3, movie:6, like:3, even:3, good:2, time:0, story:0, would:1, much:0, character:2, also:1, get:3, two:2, well:1, characters:1, first:0, see:2, way:3, make:5, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :20]):\n",
    "  print(f'{word}:{count}',end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 한국어 텍스트의 카운트 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>나는 재밌게 봄</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.14</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5점은 줄 수 없냐?</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.10.10</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.08</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>충격 결말</td>\n",
       "      <td>9</td>\n",
       "      <td>2018.10.06</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>응집력</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.05</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date   \n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29  \\\n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "5                                           나는 재밌게 봄      10  2018.10.14   \n",
       "6                                      0.5점은 줄 수 없냐?       0  2018.10.10   \n",
       "7                     헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...      10  2018.10.08   \n",
       "8                                              충격 결말       9  2018.10.06   \n",
       "9                                                응집력       8  2018.10.05   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  \n",
       "5  인피니티 워  \n",
       "6  인피니티 워  \n",
       "7  인피니티 워  \n",
       "8  인피니티 워  \n",
       "9  인피니티 워  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10점' '18' '1987' '1도' '1점' '1점도' '2시간' '2시간이' '2편' '5점' '6점' '7점' '8점'\n",
      " 'cg' 'cg가' 'cg는' 'cg도' 'cg만' 'good' 'of' 'ㅋㅋ' 'ㅋㅋㅋ' 'ㅋㅋㅋㅋ' 'ㅎㅎ' 'ㅎㅎㅎ'\n",
      " 'ㅜㅜ' 'ㅠㅠ' 'ㅠㅠㅠ' 'ㅡㅡ' '가는' '가는줄' '가면' '가서' '가슴' '가슴아픈' '가슴이' '가장' '가족'\n",
      " '가족과' '가족들과' '가족의' '가족이' '가지고' '간만에' '갈수록' '감독' '감독님' '감독은' '감독의' '감독이'\n",
      " '감동' '감동과' '감동도' '감동은' '감동을' '감동이' '감동입니다' '감동적' '감동적이고' '감동적인' '감사드립니다'\n",
      " '감사합니다' '감정이' '갑자기' '갔는데' '갔다가' '강철비' '강추' '강추합니다' '같고' '같네요' '같다' '같습니다'\n",
      " '같아' '같아요' '같은' '같은데' '같음' '같이' '개연성' '개연성이' '개인적으로' '거의' '겁나' '것도' '것은'\n",
      " '것을' '것이' '것이다' '겨울왕국' '결국' '결말' '결말이' '계속' '고맙습니다' '곤지암' '공포' '공포를'\n",
      " '공포영화' '관객']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "daum_cv = CountVectorizer(max_features=1000)\n",
    "\n",
    "#reveiew를 이용해 count vector를 학습하고, 변환\n",
    "\n",
    "daum_DTM = daum_cv.fit_transform(df.review)\n",
    "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 형태소 :  ['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
      "전체 형태소 :  ['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n",
      "전체 형태소 :  [('몰입', 'Noun'), ('할수밖에', 'Verb'), ('없다', 'Adjective'), ('.', 'Punctuation'), ('어렵게', 'Adjective'), ('생각', 'Noun'), ('할', 'Verb'), ('필요없다', 'Adjective'), ('.', 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), ('전투', 'Noun'), ('에', 'Josa'), ('참여', 'Noun'), ('한', 'Determiner'), ('듯', 'Noun'), ('손', 'Noun'), ('에', 'Josa'), ('땀', 'Noun'), ('이남', 'Noun'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Okt\n",
    "jvm_path = \"/Library/Java/JavaVirtualMachines/zulu-15.jdk/Contents/Home/bin/java\"\n",
    "twitter_tag = Okt(jvmpath=jvm_path)\n",
    "\n",
    "print('전체 형태소 : ', twitter_tag.morphs(df.review[1]))\n",
    "print('전체 형태소 : ', twitter_tag.nouns(df.review[1]))\n",
    "print('전체 형태소 : ', twitter_tag.pos(df.review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['몰입', '할수밖에', '없다', '어렵게', '생각', '할', '필요없다', '내', '전투', '참여', '듯', '손', '땀', '이남']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(doc):\n",
    "  return [\n",
    "    token for token,pos in twitter_tag.pos(doc) if pos in ['Noun','Adjective','Verb']\n",
    "  ]\n",
    "  \n",
    "print(my_tokenizer(df.review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leecoder/miniconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가' '가는' '가는줄' '가면' '가서' '가슴' '가장' '가족' '가족영화' '가지' '가치' '각색' '간' '간다'\n",
      " '간만' '갈' '갈수록' '감' '감독' '감동' '감사' '감사합니다' '감상' '감성' '감정' '감탄' '갑자기' '갔는데'\n",
      " '갔다' '갔다가' '강' '강철' '강추' '같고' '같네요' '같다' '같습니다' '같아' '같아요' '같은' '같은데'\n",
      " '같음' '개' '개그' '개봉' '개연' '개인' '거' '거기' '거리' '거의' '걱정' '건' '건가' '건지' '걸'\n",
      " '겁니다' '것' '게' '겨울왕국' '결론' '결말' '경찰' '경험' '계속' '고' '고맙습니다' '고민' '고생' '곤지암'\n",
      " '곳' '공감' '공포' '공포영화' '과' '과거' '관' '관객' '관객수' '관람' '광주' '괜찮은' '교훈' '구성'\n",
      " '국내' '국민' '군인' '군함도' '굿' '권선' '귀신' '그' '그것' '그게' '그날' '그냥' '그닥' '그대로'\n",
      " '그때' '그래픽']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 토크나이저와 특성의 최대 개수 지정\n",
    "# 명사만 추출하고 싶은 경우 tokenizer에 'twitter_tag.nouns를 바로 지정해도 됨\n",
    "\n",
    "daum_cv = CountVectorizer(max_features=1000, tokenizer=my_tokenizer)\n",
    "\n",
    "# review를 이용해 count vector를 학습하고, 변환\n",
    "daum_DTM = daum_cv.fit_transform(df.review)\n",
    "\n",
    "print(daum_cv.get_feature_names_out()[:100]) #count vector에 사용된 feature이름을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<14725x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 110800 stored elements in Compressed Sparse Row format>\n",
      "0.007524617996604414\n"
     ]
    }
   ],
   "source": [
    "print(repr(daum_DTM))\n",
    "print(110800/(14725*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내:1, 듯:1, 몰입:1, 생각:1, 손:1, 없다:1, 할:1, "
     ]
    }
   ],
   "source": [
    "for word,count in zip(daum_cv.get_feature_names_out(),daum_DTM[1].toarray()[0]):\n",
    "  if count > 0:\n",
    "    print(f'{word}:{count}',end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
